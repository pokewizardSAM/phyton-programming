{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\"\n",
        "%mkdir Extra-checkpoints\n",
        "%cd Extra-checkpoints\n",
        "!curl -l \"https://cdn-lfs.huggingface.co/repos/89/5d/895d6e86823f5538cac66a4372bd303af8d5b2e45a341d8eafdd5670c7ceba66/1f697312617db511045698dbf419ae1e2999427d4e4423a321b461cc180d1a97?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sd_xl_base_0.9.safetensors%3B+filename%3D%22sd_xl_base_0.9.safetensors%22%3B&Expires=1689901154&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTkwMTE1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OS81ZC84OTVkNmU4NjgyM2Y1NTM4Y2FjNjZhNDM3MmJkMzAzYWY4ZDViMmU0NWEzNDFkOGVhZmRkNTY3MGM3Y2ViYTY2LzFmNjk3MzEyNjE3ZGI1MTEwNDU2OThkYmY0MTlhZTFlMjk5OTQyN2Q0ZTQ0MjNhMzIxYjQ2MWNjMTgwZDFhOTc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=CTItqltllnqfa8xcd85Mnezi9nNdXwXsdC0UqypW6Uas4Z8phK7klcRP%7EL5897JvPaPANw9Z0GYQ3OBcFwEO6PwJqSwbqRgBI9KcXM00r7tC8zfgK6XtxbBCZw1pddAlaPzzAyoUZgXGpCWCfCLn%7E1Tum2hGdBP-pL5A4DReOnFsz5tikUVlQX7bsl7j4yKZGBmJqN4cFp28yGQ9GlItBaDaU6sK6RBDIVO3RzLrxrwIIY95jiZlYWHjxD9oS0QE%7E4j0ifptV9RKWJN55UlsUB%7EIPdHb4gE4drKRN02klSjWal2r2htPI%7E8p791gECxetSNFBuU1%7E9RkFTOiw8uAXQ__&Key-Pair-Id=KVTP0A1DKRTAX\" --output \"sd_xl_base_0.9.safetensors\"\n",
        "!curl -l \"https://cdn-lfs.huggingface.co/repos/c5/cf/c5cf5b9041578b2a1956fd7c90345f3c40e63afceca6d071fd25f2cbef72cd42/13406f993ca1f118e1fabf14f7e189b5271a4383512cb046887a5de27375d227?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sd_xl_refiner_0.9.safetensors%3B+filename%3D%22sd_xl_refiner_0.9.safetensors%22%3B&Expires=1689901426&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTkwMTQyNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jNS9jZi9jNWNmNWI5MDQxNTc4YjJhMTk1NmZkN2M5MDM0NWYzYzQwZTYzYWZjZWNhNmQwNzFmZDI1ZjJjYmVmNzJjZDQyLzEzNDA2Zjk5M2NhMWYxMThlMWZhYmYxNGY3ZTE4OWI1MjcxYTQzODM1MTJjYjA0Njg4N2E1ZGUyNzM3NWQyMjc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=IjLxJAonK3ygvq%7EcYhZS8pXG1N8PXOHRF6uFdSOa9Mae6aSNAVVNBk-i9X02bN1g4N26QwxyONVGACSVcz7AkJTHtnLOmygt9RJrWzAMQhQdreR3pS3r80RBCnYhHShfF81pUD0lmT74t8y7VJS3a-9403vtin6iNtyJ3wbKC7eBFsyfEH-KzBTK79ILO3lO0JtCCK2wYaTOvXaawmuRE8UcLRhZ1kJtN754nbJd0bYyZhzsZVjcddQWC-2eZh2FeHdIno83PyWQt2swdEzaVRWvesTnZGPWnyIAphxVAivMjyjs02vwutBqEhL2eM551jLb0RO9S8xZ5vTUaTvWog__&Key-Pair-Id=KVTP0A1DKRTAX\" --output \"sd_xl_refiner_0.9.safetensors\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ham7y-vibs3U",
        "outputId": "f7e025d1-95ed-4302-dbf7-17a5b67ca4d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/Extra-checkpoints\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   110  100   110    0     0    427      0 --:--:-- --:--:-- --:--:--   428\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   110  100   110    0     0    419      0 --:--:-- --:--:-- --:--:--   419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXQnxIlmiCbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7JnUb_kiFn0",
        "outputId": "c31842e3-3eaa-4d24-c223-bb686689a6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbbbbbbbb",
        "outputId": "ce8c0089-f369-4014-8169-603f3724a9c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/WORKSPACE\n",
            "/content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI\n",
            "-= Install dependencies =-\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118, https://download.pytorch.org/whl/cu117\n",
            "Collecting xformers!=0.0.18\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting torchdiffeq (from -r requirements.txt (line 2))\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Collecting torchsde (from -r requirements.txt (line 3))\n",
            "  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from -r requirements.txt (line 4))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.25.1 (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.0 (from -r requirements.txt (line 6))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.4)\n",
            "Collecting accelerate (from -r requirements.txt (line 8))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (8.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers!=0.0.18) (1.22.4)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers!=0.0.18)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers!=0.0.18)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.6)\n",
            "Collecting boltons>=20.2.1 (from torchsde->-r requirements.txt (line 3))\n",
            "  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trampoline>=0.1.2 (from torchsde->-r requirements.txt (line 3))\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.25.1->-r requirements.txt (line 5))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->-r requirements.txt (line 5))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.5.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers!=0.0.18)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: trampoline, tokenizers, safetensors, boltons, mypy-extensions, einops, typing-inspect, huggingface-hub, transformers, pyre-extensions, xformers, torchsde, torchdiffeq, accelerate\n",
            "Successfully installed accelerate-0.21.0 boltons-23.0.0 einops-0.6.1 huggingface-hub-0.16.4 mypy-extensions-1.0.0 pyre-extensions-0.0.29 safetensors-0.3.1 tokenizers-0.13.3 torchdiffeq-0.2.3 torchsde-0.2.5 trampoline-0.1.2 transformers-4.30.2 typing-inspect-0.9.0 xformers-0.0.20\n"
          ]
        }
      ],
      "source": [
        "#@title Environment Setup\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = False  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/WORKSPACE/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive/WORKSPACE\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI =-\n",
        "  !git pull\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip install xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Terminal window here (optional)\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm\n",
        "%reload_ext colabxterm"
      ],
      "metadata": {
        "id": "OBpg2LO5uuht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dddddddddd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Checkpoints\n",
        "\n",
        "# SD1.5\n",
        "# !wget -c https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -P ./models/checkpoints/\n",
        "\n",
        "# SD2\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Some SD1.5 anime style\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
        "#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# my usual models\n",
        "!wget -c https://huggingface.co/Lykon/DreamShaper/resolve/main/DreamShaper_6.31_BakedVae.safetensors -P ./models/checkpoints/\n",
        "!wget -c https://huggingface.co/OortOnline/darkSushiMixMix_225D/resolve/main/darkSushiMixMix_225D.safetensors -P ./models/checkpoints/\n",
        "!wget -c https://huggingface.co/digiplay/Photon_v1/resolve/main/photon_v1.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "\n",
        "# unCLIP models\n",
        "#!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# VAE\n",
        "# !wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/\n",
        "\n",
        "\n",
        "# Loras\n",
        "#!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/theovercomer8sContrastFix_sd21768.safetensors #theovercomer8sContrastFix SD2.x 768-v\n",
        "#!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/theovercomer8sContrastFix_sd15.safetensors #theovercomer8sContrastFix SD1.x\n",
        "\n",
        "\n",
        "# T2I-Adapter\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -P ./models/controlnet/\n",
        "\n",
        "# T2I Styles Model\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -P ./models/style_models/\n",
        "\n",
        "# CLIPVision model (needed for styles model)\n",
        "# !wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/clip_vit14.bin\n",
        "\n",
        "\n",
        "# ControlNet\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -P ./models/controlnet/\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -P ./models/controlnet/\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -P ./models/controlnet/\n",
        "!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -P ./models/controlnet/\n",
        "\n",
        "\n",
        "# Controlnet Preprocessor nodes by Fannovel16\n",
        "#!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
        "\n",
        "\n",
        "# GLIGEN\n",
        "#!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -P ./models/gligen/\n",
        "\n",
        "\n",
        "# ESRGAN upscale model\n",
        "!wget -c https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P ./models/upscale_models/\n",
        "# !wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -P ./models/upscale_models/\n",
        "!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -P ./models/upscale_models/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /\n",
        "!curl -l \"https://cdn-lfs.huggingface.co/repos/34/06/3406a0207d361144bd8b0c9c77cb08ac7ee3ba05f02f7d57caab67c52faa0b90/560424d9f68625713fc47e9e7289a98aabe1d744e1cd6a9ae5a35e9957fd127e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%274x_NMKD-Siax_200k.pth%3B+filename%3D%224x_NMKD-Siax_200k.pth%22%3B&Expires=1689922038&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTkyMjAzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC8wNi8zNDA2YTAyMDdkMzYxMTQ0YmQ4YjBjOWM3N2NiMDhhYzdlZTNiYTA1ZjAyZjdkNTdjYWFiNjdjNTJmYWEwYjkwLzU2MDQyNGQ5ZjY4NjI1NzEzZmM0N2U5ZTcyODlhOThhYWJlMWQ3NDRlMWNkNmE5YWU1YTM1ZTk5NTdmZDEyN2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qTvlO1ybdlBBQlUpACVwrHFvexW6GVGdItgmaKnaB-xk%7EEa%7EOrR71nmYS3ATQi-7-UkuE%7EhrtSjeCXKKpOAu5ZeZfsAn4oWqbrpVILfbwLjVTb-K30i1gA1976nl3nUdnltHHRQervuiGf0njgQOKOvvZt%7EZPnJ6AJdwR%7E1yEgNpavpMZFsR2sJ9V7TRG3CiBoau3uG6zR2vfdvzQXQnvOdsvSrEaz-QclONv4ZyKMV5y1AgYNk7Z9WJv964wqkzR2YFS3L24SP66zlnrYcwwzHB0KjK-io35%7E2s6botFKjgioTuFM-Kf-LUIj6K48JPv3-BtG03FKAMrdJezYigrQ__&Key-Pair-Id=KVTP0A1DKRTAX\" --output /content/drive/MyDrive/WORKSPACE/ComfyUI/models/upscale_models/4x_NMKD-Siax_200k.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zbjz_iYw9CI",
        "outputId": "08b39de9-08a5-40e2-88a9-e2a062e9df5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 63.8M  100 63.8M    0     0  89.5M      0 --:--:-- --:--:-- --:--:-- 89.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6f1c8c-adbd-4edd-b9bb-84da1438c5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI\n",
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 0.842s\n",
            "\n",
            "Prestartup times for custom nodes:\n",
            "   0.0 seconds: /content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI/custom_nodes/comfyuiManager_v012.zip (Unzipped Files)\n",
            "\n",
            "Total VRAM 15102 MB, total RAM 12983 MB\n",
            "Enabling highvram mode because your GPU has more vram than your computer has ram. If you don't want this use: --normalvram\n",
            "xformers version: 0.0.20\n",
            "2023-07-18 06:56:29.520759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Set vram state to: HIGH_VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "Using xformers cross attention\n",
            "Adding extra search path checkpoints /content/Extra-checkpoints\n",
            "Adding extra search path configs /content/models/Stable-diffusion\n",
            "Adding extra search path vae /content/models/VAE\n",
            "Adding extra search path loras /content/models/Lora\n",
            "Adding extra search path loras /content/models/LyCORIS\n",
            "Adding extra search path upscale_models /content/models/ESRGAN\n",
            "Adding extra search path upscale_models /content/models/RealESRGAN\n",
            "Adding extra search path upscale_models /content/models/SwinIR\n",
            "Adding extra search path embeddings /content/embeddings\n",
            "Adding extra search path hypernetworks /content/models/hypernetworks\n",
            "Adding extra search path controlnet /content/models/ControlNet\n",
            "### Loading: ComfyUI-Manager (V0.12)\n",
            "### ComfyUI Revision: 1176 [1679abd8]\n",
            "\n",
            "ComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\n",
            "\n",
            "\n",
            "Import times for custom nodes:\n",
            "   0.0 seconds: /content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI/custom_nodes/ComfyUI-Image-Selector\n",
            "   0.0 seconds: /content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI/custom_nodes/SeargeSDXL\n",
            "   0.2 seconds: /content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI/custom_nodes/comfyuiManager_v012.zip (Unzipped Files)\n",
            "\n",
            "The password/enpoint ip for localtunnel is: 34.168.120.249\n",
            "your url is: https://polite-cougars-buy.loca.lt\n",
            "got prompt\n",
            "Failed to validate prompt for output 190:\n",
            "* UpscaleModelLoader 194:\n",
            "  - Value not in list: model_name: 'x1_ITF_SkinDiffDetail_Lite_v1.pth' not in ['4x_NMKD-Siax_200k.pth', 'RealESRGAN_x4.pth', 'RealESRGAN_x4plus.pth']\n",
            "Output will be ignored\n",
            "Failed to validate prompt for output 196:\n",
            "* UpscaleModelLoader 198:\n",
            "  - Value not in list: model_name: '4x-UltraSharp.pth' not in ['4x_NMKD-Siax_200k.pth', 'RealESRGAN_x4.pth', 'RealESRGAN_x4plus.pth']\n",
            "Output will be ignored\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "model_type EPS\n",
            "adm 2816\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "left over keys: dict_keys(['denoiser.log_sigmas', 'denoiser.sigmas'])\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 4096 4096\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 4096 4096\n",
            "100% 20/20 [00:18<00:00,  1.10it/s]\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "model_type EPS\n",
            "adm 2560\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "left over keys: dict_keys(['denoiser.log_sigmas', 'denoiser.sigmas'])\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 6.0\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 2.5\n",
            "100% 5/5 [00:05<00:00,  1.00s/it]\n",
            "Prompt executed in 149.19 seconds\n",
            "\n",
            "Stopped server\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI/main.py\", line 178, in <module>\n",
            "    cleanup_temp()\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1EVMqDo1u1cobcOUQ3d21QeMVRVahadDz/ComfyUI/main.py\", line 110, in cleanup_temp\n",
            "    temp_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"temp\")\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
            "    st = os.lstat(newpath)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/WORKSPACE/ComfyUI\n",
        "!npm install -g localtunnel\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "# extra-libraries:\n",
        "# !pip install insightface\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
        "\n",
        "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
        "  for line in p.stdout:\n",
        "    print(line.decode(), end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggggggggg"
      },
      "source": [
        "### Run ComfyUI with colab iframe (use only in case the previous way with localtunnel doesn't work)\n",
        "\n",
        "You should see the ui appear in an iframe. If you get a 403 error, it's your firefox settings or an extension that's messing things up.\n",
        "\n",
        "If you want to open it in another window use the link.\n",
        "\n",
        "Note that some UI features like live image previews won't work because the colab iframe blocks websockets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhhhhhhhh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown Run me dammit\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "  print(\"to open it in a window you can open this link here:\")\n",
        "  output.serve_kernel_port_as_window(port)\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}